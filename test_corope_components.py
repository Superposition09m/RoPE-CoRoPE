"""
Co-RoPE ç»„ä»¶çº§æµ‹è¯•
ç±»ä¼¼äº test_descriptor_to_pointer_rope.py çš„é£æ ¼
é€ä¸ªéªŒè¯ Co-RoPE Triton å®ç°çš„å„ä¸ªå…³é”®ç»„ä»¶
"""

import torch
import triton
import triton.language as tl

DEVICE = triton.runtime.driver.active.get_active_torch_device()

def is_cuda():
    return triton.runtime.driver.active.get_current_target().backend == "cuda"

def supports_host_descriptor():
    return is_cuda() and torch.cuda.get_device_capability()[0] >= 9


# ========================================
# æµ‹è¯• 1: inv_freq è®¡ç®—
# ========================================
@triton.jit
def _test_inv_freq_kernel(
    inv_freq_out,
    theta,
    HEAD_DIM: tl.constexpr,
):
    """æµ‹è¯• inv_freq çš„åŠ¨æ€è®¡ç®—"""
    half_dim: tl.constexpr = HEAD_DIM // 2
    offs_d = tl.arange(0, half_dim)
    
    # è®¡ç®— inv_freq = 1.0 / (theta ** (2*offs_d / HEAD_DIM))
    # æ³¨æ„ï¼šPyTorch ä½¿ç”¨ arange(0, HEAD_DIM, 2)ï¼Œå³ [0, 2, 4, 6, ...]
    exponent = (2 * offs_d).to(tl.float32) / HEAD_DIM
    inv_freq = 1.0 / tl.exp(exponent * tl.log(theta))
    
    # å†™å›
    tl.store(inv_freq_out + offs_d, inv_freq)


def test_inv_freq():
    """æµ‹è¯• inv_freq è®¡ç®—çš„æ­£ç¡®æ€§"""
    print("="*60)
    print("æµ‹è¯• 1: inv_freq åŠ¨æ€è®¡ç®—")
    print("="*60)
    
    HEAD_DIM = 64
    theta = 10000.0
    half_dim = HEAD_DIM // 2
    
    # Triton è®¡ç®—
    inv_freq_triton = torch.zeros(half_dim, dtype=torch.float32, device=DEVICE)
    _test_inv_freq_kernel[(1,)](inv_freq_triton, theta, HEAD_DIM)
    
    # PyTorch å‚è€ƒ
    inv_freq_ref = 1.0 / (theta ** (torch.arange(0, HEAD_DIM, 2, device=DEVICE).float() / HEAD_DIM))
    
    # å¯¹æ¯”
    diff = (inv_freq_triton - inv_freq_ref).abs().max().item()
    print(f"  Triton: {inv_freq_triton[:5].tolist()}")
    print(f"  PyTorch: {inv_freq_ref[:5].tolist()}")
    print(f"  Max Diff: {diff:.2e}")
    
    if diff < 1e-5:
        print("  âœ… PASS: inv_freq è®¡ç®—æ­£ç¡®")
        return True
    else:
        print("  âŒ FAIL: inv_freq è®¡ç®—æœ‰è¯¯")
        return False


# ========================================
# æµ‹è¯• 2: Phase 1 é‡Œç¨‹è®¡ç®—
# ========================================
@triton.jit
def _test_mileage_phase1_kernel(
    Q, K,
    a_tt_out,
    sm_scale,
    N_CTX, HEAD_DIM: tl.constexpr,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    stride_q_seq, stride_q_dim,
    stride_k_seq, stride_k_dim,
):
    """æµ‹è¯• Phase 1 çš„é‡Œç¨‹è®¡ç®—é€»è¾‘"""
    pid = tl.program_id(0)
    
    half_dim: tl.constexpr = HEAD_DIM // 2
    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_d_first = tl.arange(0, half_dim)
    offs_d_second = offs_d_first + half_dim
    
    # Load Q
    mask_q = (offs_m[:, None] < N_CTX)
    q1_ptrs = Q + offs_m[:, None] * stride_q_seq + offs_d_first[None, :] * stride_q_dim
    q2_ptrs = Q + offs_m[:, None] * stride_q_seq + offs_d_second[None, :] * stride_q_dim
    q1 = tl.load(q1_ptrs, mask=mask_q, other=0.0)
    q2 = tl.load(q2_ptrs, mask=mask_q, other=0.0)
    
    # Initialize diagonal cumulative mileage
    a_tt = tl.zeros([BLOCK_M], dtype=tl.float32)
    
    # Scan all K blocks
    for start_n in range(0, N_CTX, BLOCK_N):
        offs_n_curr = start_n + tl.arange(0, BLOCK_N)
        mask_k = (offs_n_curr[:, None] < N_CTX)
        
        # Load K
        k1_ptrs = K + offs_n_curr[:, None] * stride_k_seq + offs_d_first[None, :] * stride_k_dim
        k2_ptrs = K + offs_n_curr[:, None] * stride_k_seq + offs_d_second[None, :] * stride_k_dim
        k1 = tl.load(k1_ptrs, mask=mask_k, other=0.0)
        k2 = tl.load(k2_ptrs, mask=mask_k, other=0.0)
        
        # Raw dot product (no RoPE)
        qk_raw = tl.dot(q1, tl.trans(k1)) + tl.dot(q2, tl.trans(k2))
        
        # z = sigmoid(qk * sm_scale)
        z_block = tl.sigmoid(qk_raw * sm_scale)
        
        # Accumulate only diagonal and below
        mask_diagonal = offs_m[:, None] >= offs_n_curr[None, :]
        z_masked = tl.where(mask_diagonal, z_block, 0.0)
        
        # Sum across K dimension
        a_tt = a_tt + tl.sum(z_masked, axis=1)
    
    # Write back
    mask_out = (offs_m < N_CTX)
    tl.store(a_tt_out + offs_m, a_tt, mask=mask_out)


def test_mileage_phase1():
    """æµ‹è¯• Phase 1 é‡Œç¨‹è®¡ç®—æ˜¯å¦ä¸ PyTorch ä¸€è‡´"""
    print("\n" + "="*60)
    print("æµ‹è¯• 2: Phase 1 é‡Œç¨‹è®¡ç®—")
    print("="*60)
    
    N_CTX = 128
    HEAD_DIM = 64
    BLOCK_M = 64
    BLOCK_N = 64
    sm_scale = (HEAD_DIM ** -0.5)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    q = torch.randn((N_CTX, HEAD_DIM), dtype=torch.float16, device=DEVICE)
    k = torch.randn((N_CTX, HEAD_DIM), dtype=torch.float16, device=DEVICE)
    
    # Triton è®¡ç®—
    a_tt_triton = torch.zeros(N_CTX, dtype=torch.float32, device=DEVICE)
    grid = (triton.cdiv(N_CTX, BLOCK_M),)
    _test_mileage_phase1_kernel[grid](
        q, k, a_tt_triton, sm_scale,
        N_CTX, HEAD_DIM, BLOCK_M, BLOCK_N,
        q.stride(0), q.stride(1),
        k.stride(0), k.stride(1),
    )
    
    # PyTorch å‚è€ƒ
    qk = torch.matmul(q, k.T) * sm_scale  # [N_CTX, N_CTX]
    z = torch.sigmoid(qk)
    
    # å¯¹è§’çº¿ç´¯ç§¯ï¼ša_tt[i] = sum_{j=0}^{i} z[i,j]
    # ä½¿ç”¨ tril mask
    mask_tril = torch.tril(torch.ones(N_CTX, N_CTX, device=DEVICE, dtype=torch.bool))
    z_masked = torch.where(mask_tril, z, 0.0)
    a_tt_ref = z_masked.sum(dim=1)
    
    # å¯¹æ¯”
    diff = (a_tt_triton - a_tt_ref).abs().max().item()
    print(f"  Triton a_tt èŒƒå›´: [{a_tt_triton.min().item():.4f}, {a_tt_triton.max().item():.4f}]")
    print(f"  PyTorch a_tt èŒƒå›´: [{a_tt_ref.min().item():.4f}, {a_tt_ref.max().item():.4f}]")
    print(f"  Max Diff: {diff:.2e}")
    
    if diff < 1e-3:
        print("  âœ… PASS: Phase 1 é‡Œç¨‹è®¡ç®—æ­£ç¡®")
        return True
    else:
        print("  âŒ FAIL: Phase 1 é‡Œç¨‹è®¡ç®—æœ‰è¯¯")
        # è¯¦ç»†å¯¹æ¯”å‰å‡ ä¸ªå…ƒç´ 
        print(f"\n  è¯¦ç»†å¯¹æ¯” (å‰10ä¸ªå…ƒç´ ):")
        for i in range(min(10, N_CTX)):
            print(f"    a_tt[{i}]: Triton={a_tt_triton[i].item():.6f}, PyTorch={a_tt_ref[i].item():.6f}, diff={abs(a_tt_triton[i].item()-a_tt_ref[i].item()):.2e}")
        return False


# ========================================
# æµ‹è¯• 3: tl.cumsum è¡Œä¸ºéªŒè¯
# ========================================
@triton.jit
def _test_cumsum_kernel(
    X,
    Y_out,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
):
    """æµ‹è¯• tl.cumsum çš„è¡Œä¸º"""
    pid = tl.program_id(0)
    
    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_n = tl.arange(0, BLOCK_N)
    
    # Load
    x = tl.load(X + offs_m[:, None] * BLOCK_N + offs_n[None, :])
    
    # Cumsum along axis=1
    y = tl.cumsum(x, axis=1)
    
    # Store
    tl.store(Y_out + offs_m[:, None] * BLOCK_N + offs_n[None, :], y)


def test_cumsum_behavior():
    """éªŒè¯ tl.cumsum çš„è¡Œä¸ºä¸ torch.cumsum ä¸€è‡´"""
    print("\n" + "="*60)
    print("æµ‹è¯• 3: tl.cumsum è¡Œä¸ºéªŒè¯")
    print("="*60)
    
    BLOCK_M = 4
    BLOCK_N = 8
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn((BLOCK_M, BLOCK_N), dtype=torch.float32, device=DEVICE)
    
    # Triton è®¡ç®—
    y_triton = torch.zeros_like(x)
    _test_cumsum_kernel[(1,)](x, y_triton, BLOCK_M, BLOCK_N)
    
    # PyTorch å‚è€ƒ
    y_ref = torch.cumsum(x, dim=1)
    
    # å¯¹æ¯”
    diff = (y_triton - y_ref).abs().max().item()
    print(f"  Input:\n{x}")
    print(f"\n  Triton cumsum:\n{y_triton}")
    print(f"\n  PyTorch cumsum:\n{y_ref}")
    print(f"\n  Max Diff: {diff:.2e}")
    
    if diff < 1e-5:
        print("  âœ… PASS: tl.cumsum è¡Œä¸ºæ­£ç¡®")
        return True
    else:
        print("  âŒ FAIL: tl.cumsum è¡Œä¸ºå¼‚å¸¸")
        return False


# ========================================
# æµ‹è¯• 4: Co-RoPE èƒ½é‡åœºè®¡ç®—
# ========================================
@triton.jit
def _test_energy_field_kernel(
    Q1, Q2, K1, K2,
    delta_a, inv_freq,
    qk_out,
    BLOCK_M: tl.constexpr,
    BLOCK_N: tl.constexpr,
    HEAD_DIM: tl.constexpr,
):
    """æµ‹è¯• Co-RoPE èƒ½é‡åœºå’Œç›¸ä½è°ƒåˆ¶çš„è®¡ç®—"""
    half_dim: tl.constexpr = HEAD_DIM // 2
    
    # Load data
    offs_m = tl.arange(0, BLOCK_M)
    offs_n = tl.arange(0, BLOCK_N)
    offs_d = tl.arange(0, half_dim)
    
    q1 = tl.load(Q1 + offs_m[:, None] * half_dim + offs_d[None, :])
    q2 = tl.load(Q2 + offs_m[:, None] * half_dim + offs_d[None, :])
    k1 = tl.load(K1 + offs_n[:, None] * half_dim + offs_d[None, :])
    k2 = tl.load(K2 + offs_n[:, None] * half_dim + offs_d[None, :])
    
    delta = tl.load(delta_a + offs_m[:, None] * BLOCK_N + offs_n[None, :])
    inv_f = tl.load(inv_freq + offs_d)
    
    # Compute phi
    phi = delta[:, :, None] * inv_f[None, None, :]  # [BLOCK_M, BLOCK_N, half_dim]
    cos_phi = tl.cos(phi)
    sin_phi = tl.sin(phi)
    
    # Compute energy fields
    E_A = q1[:, None, :] * k1[None, :, :] + q2[:, None, :] * k2[None, :, :]
    E_B = q2[:, None, :] * k1[None, :, :] - q1[:, None, :] * k2[None, :, :]
    
    # Co-RoPE score
    qk = tl.sum(E_A * cos_phi - E_B * sin_phi, axis=2)
    
    # Store
    tl.store(qk_out + offs_m[:, None] * BLOCK_N + offs_n[None, :], qk)


def test_energy_field():
    """æµ‹è¯• Co-RoPE èƒ½é‡åœºè®¡ç®—"""
    print("\n" + "="*60)
    print("æµ‹è¯• 4: Co-RoPE èƒ½é‡åœºå’Œç›¸ä½è°ƒåˆ¶")
    print("="*60)
    
    BLOCK_M = 4
    BLOCK_N = 8
    HEAD_DIM = 64
    half_dim = HEAD_DIM // 2
    theta = 10000.0
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    q = torch.randn((BLOCK_M, HEAD_DIM), dtype=torch.float32, device=DEVICE)
    k = torch.randn((BLOCK_N, HEAD_DIM), dtype=torch.float32, device=DEVICE)
    delta_a = torch.randn((BLOCK_M, BLOCK_N), dtype=torch.float32, device=DEVICE)
    
    q1, q2 = q[:, :half_dim], q[:, half_dim:]
    k1, k2 = k[:, :half_dim], k[:, half_dim:]
    
    inv_freq = 1.0 / (theta ** (torch.arange(0, HEAD_DIM, 2, device=DEVICE).float() / HEAD_DIM))
    
    # Triton è®¡ç®—
    qk_triton = torch.zeros((BLOCK_M, BLOCK_N), dtype=torch.float32, device=DEVICE)
    _test_energy_field_kernel[(1,)](
        q1.contiguous(), q2.contiguous(),
        k1.contiguous(), k2.contiguous(),
        delta_a.contiguous(), inv_freq,
        qk_triton,
        BLOCK_M, BLOCK_N, HEAD_DIM,
    )
    
    # PyTorch å‚è€ƒ
    phi = delta_a.unsqueeze(-1) * inv_freq.view(1, 1, -1)  # [BLOCK_M, BLOCK_N, half_dim]
    cos_phi = torch.cos(phi)
    sin_phi = torch.sin(phi)
    
    E_A = q1.unsqueeze(1) * k1.unsqueeze(0) + q2.unsqueeze(1) * k2.unsqueeze(0)
    E_B = q2.unsqueeze(1) * k1.unsqueeze(0) - q1.unsqueeze(1) * k2.unsqueeze(0)
    
    qk_ref = (E_A * cos_phi - E_B * sin_phi).sum(dim=-1)
    
    # å¯¹æ¯”
    diff = (qk_triton - qk_ref).abs().max().item()
    print(f"  Triton qk èŒƒå›´: [{qk_triton.min().item():.4f}, {qk_triton.max().item():.4f}]")
    print(f"  PyTorch qk èŒƒå›´: [{qk_ref.min().item():.4f}, {qk_ref.max().item():.4f}]")
    print(f"  Max Diff: {diff:.2e}")
    
    if diff < 1e-4:
        print("  âœ… PASS: èƒ½é‡åœºè®¡ç®—æ­£ç¡®")
        return True
    else:
        print("  âŒ FAIL: èƒ½é‡åœºè®¡ç®—æœ‰è¯¯")
        print(f"\n  è¯¦ç»†å¯¹æ¯” (ç¬¬ä¸€è¡Œ):")
        print(f"    Triton:  {qk_triton[0, :].tolist()}")
        print(f"    PyTorch: {qk_ref[0, :].tolist()}")
        return False


# ========================================
# æµ‹è¯• 5: å¢é‡ç´¯ç§¯ + tl.cumsum
# ========================================
@triton.jit
def _test_incremental_cumsum_kernel(
    Q1, Q2, K1, K2,
    a_tt_in,
    delta_a_out,
    sm_scale,
    N_CTX, BLOCK_M: tl.constexpr, BLOCK_N: tl.constexpr, HEAD_DIM: tl.constexpr,
    stride_q_seq, stride_q_dim,
    stride_k_seq, stride_k_dim,
):
    """æµ‹è¯•å¢é‡ç´¯ç§¯ + tl.cumsum çš„ç»„åˆé€»è¾‘"""
    pid = tl.program_id(0)
    
    half_dim: tl.constexpr = HEAD_DIM // 2
    offs_m = pid * BLOCK_M + tl.arange(0, BLOCK_M)
    offs_d_first = tl.arange(0, half_dim)
    
    # Load Q
    mask_q = (offs_m[:, None] < N_CTX)
    q1 = tl.load(Q1 + offs_m[:, None] * half_dim + offs_d_first[None, :], mask=mask_q, other=0.0)
    q2 = tl.load(Q2 + offs_m[:, None] * half_dim + offs_d_first[None, :], mask=mask_q, other=0.0)
    
    # Load a_tt
    a_tt = tl.load(a_tt_in + offs_m)
    
    # Simulate Phase 2: loop over K blocks
    a_cum = tl.zeros([BLOCK_M], dtype=tl.float32)
    
    block_idx = 0
    for start_n in range(0, N_CTX, BLOCK_N):
        offs_n = start_n + tl.arange(0, BLOCK_N)
        mask_k = (offs_n[:, None] < N_CTX)
        
        # Load K
        k1 = tl.load(K1 + offs_n[:, None] * half_dim + offs_d_first[None, :], mask=mask_k, other=0.0)
        k2 = tl.load(K2 + offs_n[:, None] * half_dim + offs_d_first[None, :], mask=mask_k, other=0.0)
        
        # Compute mileage
        qk_mile = tl.dot(q1, tl.trans(k1)) + tl.dot(q2, tl.trans(k2))
        z_tile = tl.sigmoid(qk_mile * sm_scale)
        
        # Cumsum within block
        z_cumsum = tl.cumsum(z_tile, axis=1)
        
        # Current accumulated mileage
        a_current = a_cum[:, None] + z_cumsum
        
        # Compute delta_a
        delta = a_tt[:, None] - a_current
        
        # Store first block's delta_a for verification
        if block_idx == 0:
            mask_store = (offs_m[:, None] < N_CTX) & (offs_n[None, :] < BLOCK_N)
            tl.store(delta_a_out + offs_m[:, None] * BLOCK_N + offs_n[None, :], delta, mask=mask_store)
        
        # Update cumulative
        a_cum = a_cum + tl.sum(z_tile, axis=1)
        
        block_idx += 1


def test_incremental_cumsum():
    """æµ‹è¯•å¢é‡ç´¯ç§¯ + tl.cumsum çš„æ­£ç¡®æ€§"""
    print("\n" + "="*60)
    print("æµ‹è¯• 5: å¢é‡ç´¯ç§¯ + tl.cumsum")
    print("="*60)
    
    N_CTX = 128
    HEAD_DIM = 64
    BLOCK_M = 64
    BLOCK_N = 64
    half_dim = HEAD_DIM // 2
    sm_scale = (HEAD_DIM ** -0.5)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    q = torch.randn((N_CTX, HEAD_DIM), dtype=torch.float32, device=DEVICE)
    k = torch.randn((N_CTX, HEAD_DIM), dtype=torch.float32, device=DEVICE)
    q1, q2 = q[:, :half_dim].contiguous(), q[:, half_dim:].contiguous()
    k1, k2 = k[:, :half_dim].contiguous(), k[:, half_dim:].contiguous()
    
    # å…ˆè®¡ç®— a_tt (ä½¿ç”¨ PyTorch)
    qk = torch.matmul(q, k.T) * sm_scale
    z = torch.sigmoid(qk)
    mask_tril = torch.tril(torch.ones(N_CTX, N_CTX, device=DEVICE, dtype=torch.bool))
    a_tt_ref = torch.where(mask_tril, z, 0.0).sum(dim=1)
    
    # Triton è®¡ç®—
    delta_a_triton = torch.zeros((N_CTX, BLOCK_N), dtype=torch.float32, device=DEVICE)
    grid = (triton.cdiv(N_CTX, BLOCK_M),)
    _test_incremental_cumsum_kernel[grid](
        q1, q2, k1, k2,
        a_tt_ref,
        delta_a_triton,
        sm_scale,
        N_CTX, BLOCK_M, BLOCK_N, HEAD_DIM,
        half_dim, 1,
        half_dim, 1,
    )
    
    # PyTorch å‚è€ƒ (åªéªŒè¯ç¬¬ä¸€ä¸ªblock)
    # è®¡ç®—ç¬¬ä¸€ä¸ª block çš„ cumsum
    z_first_block = z[:, :BLOCK_N]
    z_cumsum_ref = torch.cumsum(z_first_block, dim=1)
    a_current_ref = z_cumsum_ref  # ç¬¬ä¸€ä¸ª blockï¼Œa_cum=0
    delta_a_ref = a_tt_ref[:, None] - a_current_ref
    
    # å¯¹æ¯”
    diff = (delta_a_triton - delta_a_ref).abs().max().item()
    print(f"  Max Diff: {diff:.2e}")
    
    if diff < 1e-4:
        print("  âœ… PASS: å¢é‡ç´¯ç§¯ + cumsum é€»è¾‘æ­£ç¡®")
        return True
    else:
        print("  âŒ FAIL: å¢é‡ç´¯ç§¯ + cumsum é€»è¾‘æœ‰è¯¯")
        print(f"\n  è¯¦ç»†å¯¹æ¯” (ç¬¬ä¸€è¡Œ):")
        print(f"    Triton:  {delta_a_triton[0, :].tolist()}")
        print(f"    PyTorch: {delta_a_ref[0, :].tolist()}")
        return False


# ========================================
# Main Test Suite
# ========================================
if __name__ == "__main__":
    print("\n" + "ğŸ§ª "*30)
    print("Co-RoPE ç»„ä»¶çº§æµ‹è¯•å¥—ä»¶")
    print("ğŸ§ª "*30 + "\n")
    
    results = []
    
    # Test 1: inv_freq
    results.append(("inv_freq è®¡ç®—", test_inv_freq()))
    
    # Test 2: Phase 1 mileage
    results.append(("Phase 1 é‡Œç¨‹è®¡ç®—", test_mileage_phase1()))
    
    # Test 3: tl.cumsum
    results.append(("tl.cumsum è¡Œä¸º", test_cumsum_behavior()))
    
    # Test 4: Energy field
    results.append(("èƒ½é‡åœºè®¡ç®—", test_energy_field()))
    
    # Test 5: Incremental cumsum
    results.append(("å¢é‡ç´¯ç§¯ + cumsum", test_incremental_cumsum()))
    
    # Summary
    print("\n" + "="*60)
    print("æµ‹è¯•æ€»ç»“")
    print("="*60)
    for name, passed in results:
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"  {status}: {name}")
    
    all_passed = all(p for _, p in results)
    print("\n" + "="*60)
    if all_passed:
        print("âœ… æ‰€æœ‰ç»„ä»¶æµ‹è¯•é€šè¿‡ï¼")
        print("ğŸ“ ä¸‹ä¸€æ­¥ï¼šé›†æˆåˆ°å®Œæ•´çš„ Co-RoPE Attention kernel")
    else:
        print("âŒ éƒ¨åˆ†ç»„ä»¶æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦ä¿®å¤")
    print("="*60 + "\n")

